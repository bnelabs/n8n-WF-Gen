# =============================================================================
# AI Provider Configuration
# =============================================================================
# Set AI_PROVIDER to choose which AI service to use
# Options: gemini, openai, anthropic, openrouter, ollama, lmstudio
AI_PROVIDER=gemini

# =============================================================================
# Google Gemini Configuration
# =============================================================================
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash-preview-04-17
GEMINI_TEMPERATURE=0.2
GEMINI_MAX_TOKENS=4096

# =============================================================================
# OpenAI Configuration
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_TEMPERATURE=0.2
OPENAI_MAX_TOKENS=4096

# =============================================================================
# Anthropic Claude Configuration
# =============================================================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_TEMPERATURE=0.2
ANTHROPIC_MAX_TOKENS=4096

# =============================================================================
# OpenRouter Configuration
# =============================================================================
# Get your API key from: https://openrouter.ai/keys
# OpenRouter provides access to many models through a single API
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_MODEL=openai/gpt-4-turbo-preview
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_TEMPERATURE=0.2
OPENROUTER_MAX_TOKENS=4096

# =============================================================================
# Ollama Configuration (Local Models)
# =============================================================================
# Install Ollama from: https://ollama.ai/
# Popular models: llama2, mistral, codellama, llama3, phi3
# Pull a model with: ollama pull llama2
OLLAMA_MODEL=llama2
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.2
OLLAMA_MAX_TOKENS=4096
OLLAMA_TIMEOUT=120000

# =============================================================================
# LM Studio Configuration (Local Models)
# =============================================================================
# Install LM Studio from: https://lmstudio.ai/
# Start the local server in LM Studio and load a model
LMSTUDIO_MODEL=local-model
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_TEMPERATURE=0.2
LMSTUDIO_MAX_TOKENS=4096

# =============================================================================
# Server Configuration
# =============================================================================
PORT=3001
FRONTEND_URL=http://localhost:5173
APP_URL=http://localhost:5173

# For production, set FRONTEND_URL to your production domain
# FRONTEND_URL=https://yourdomain.com
# APP_URL=https://yourdomain.com
